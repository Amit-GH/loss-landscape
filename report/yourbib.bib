
@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@inproceedings{borsch2011,
	Address = {Canberra, Australia},
	Author = {Benjamin Borschinger and Mark Johnson},
	Booktitle = {Proceedings of the Australasian Language Technology Association Workshop 2011},
	Month = {December},
	Pages = {10--18},
	Title = {A Particle Filter algorithm for {B}ayesian Wordsegmentation},
	Year = {2011}}


@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
   publisher = {American Psychological Association},
   address = {Washington, DC}
}

@article{ACM:83,
	author = {Association for Computing Machinery},
	year = "1983",
	journal = {Computing Reviews},
	volume = "24",
	number = "11",
	pages = "503--512"
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@misc{cai2019effective,
    title={Effective and Efficient Dropout for Deep Convolutional Neural Networks},
    author={Shaofeng Cai and Yao Shu and Gang Chen and Beng Chin Ooi and Wei Wang and Meihui Zhang},
    year={2019},
    eprint={1904.03392},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@article{DBLP:journals/corr/abs-1712-09913,
  author    = {Hao Li and
               Zheng Xu and
               Gavin Taylor and
               Tom Goldstein},
  title     = {Visualizing the Loss Landscape of Neural Nets},
  journal   = {CoRR},
  volume    = {abs/1712.09913},
  year      = {2017},
  url       = {http://arxiv.org/abs/1712.09913},
  archivePrefix = {arXiv},
  eprint    = {1712.09913},
  timestamp = {Thu, 16 May 2019 13:19:49 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1712-09913.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/HelmboldL16,
  author    = {David P. Helmbold and
               Philip M. Long},
  title     = {Dropout Versus Weight Decay for Deep Networks},
  journal   = {CoRR},
  volume    = {abs/1602.04484},
  year      = {2016},
  url       = {http://arxiv.org/abs/1602.04484},
  archivePrefix = {arXiv},
  eprint    = {1602.04484},
  timestamp = {Mon, 13 Aug 2018 16:48:43 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/HelmboldL16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{conf/nips/BaldiS13,
  added-at = {2020-03-06T00:00:00.000+0100},
  author = {Baldi, Pierre and Sadowski, Peter J.},
  biburl = {https://www.bibsonomy.org/bibtex/2e5464707872b8af862020fc9363e9f37/dblp},
  booktitle = {NIPS},
  crossref = {conf/nips/2013},
  editor = {Burges, Christopher J. C. and Bottou, Léon and Ghahramani, Zoubin and Weinberger, Kilian Q.},
  ee = {http://papers.nips.cc/paper/4878-understanding-dropout},
  interhash = {e48fb23e0b36f0a3460f1a4cccbf1b36},
  intrahash = {e5464707872b8af862020fc9363e9f37},
  keywords = {dblp},
  pages = {2814-2822},
  timestamp = {2020-03-07T11:48:47.000+0100},
  title = {Understanding Dropout.},
  url = {http://dblp.uni-trier.de/db/conf/nips/nips2013.html#BaldiS13},
  year = 2013
}


@misc{li2018visualizing,
      title={Visualizing the Loss Landscape of Neural Nets}, 
      author={Hao Li and Zheng Xu and Gavin Taylor and Christoph Studer and Tom Goldstein},
      year={2018},
      eprint={1712.09913},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@incollection{NIPS2018_7515,
title = {How Does Batch Normalization Help Optimization?},
author = {Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Madry, Aleksander},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {2483--2493},
year = {2018},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf}
}
@misc{krueger2017zoneout,
      title={Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations}, 
      author={David Krueger and Tegan Maharaj and János Kramár and Mohammad Pezeshki and Nicolas Ballas and Nan Rosemary Ke and Anirudh Goyal and Yoshua Bengio and Aaron Courville and Chris Pal},
      year={2017},
      eprint={1606.01305},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}
@misc{semeniuta2016recurrent,
      title={Recurrent Dropout without Memory Loss}, 
      author={Stanislau Semeniuta and Aliaksei Severyn and Erhardt Barth},
      year={2016},
      eprint={1603.05118},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{gal2016theoretically,
      title={A Theoretically Grounded Application of Dropout in Recurrent Neural Networks}, 
      author={Yarin Gal and Zoubin Ghahramani},
      year={2016},
      eprint={1512.05287},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{neal2019modern,
      title={A Modern Take on the Bias-Variance Tradeoff in Neural Networks}, 
      author={Brady Neal and Sarthak Mittal and Aristide Baratin and Vinayak Tantia and Matthew Scicluna and Simon Lacoste-Julien and Ioannis Mitliagkas},
      year={2019},
      eprint={1810.08591},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{yang2020rethinking,
      title={Rethinking Bias-Variance Trade-off for Generalization of Neural Networks}, 
      author={Zitong Yang and Yaodong Yu and Chong You and Jacob Steinhardt and Yi Ma},
      year={2020},
      eprint={2002.11328},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{qi2020variational,
      title={Variational Inference-Based Dropout in Recurrent Neural Networks for Slot Filling in Spoken Language Understanding}, 
      author={Jun Qi and Xu Liu and Javier Tejedor},
      year={2020},
      eprint={2009.01003},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@InProceedings{maas-EtAl:2011:ACL-HLT2011,
  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},
  title     = {Learning Word Vectors for Sentiment Analysis},
  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
  month     = {June},
  year      = {2011},
  address   = {Portland, Oregon, USA},
  publisher = {Association for Computational Linguistics},
  pages     = {142--150},
  url       = {http://www.aclweb.org/anthology/P11-1015}
}

 @InProceedings{pmlr-v28-wan13, title = {Regularization of Neural Networks using DropConnect}, author = {Li Wan and Matthew Zeiler and Sixin Zhang and Yann Le Cun and Rob Fergus}, booktitle = {Proceedings of the 30th International Conference on Machine Learning}, pages = {1058--1066}, year = {2013}, editor = {Sanjoy Dasgupta and David McAllester}, volume = {28}, number = {3}, series = {Proceedings of Machine Learning Research}, address = {Atlanta, Georgia, USA}, month = {17--19 Jun}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v28/wan13.pdf}, url = {http://proceedings.mlr.press/v28/wan13.html}, abstract = {We introduce DropConnect, a generalization of DropOut, for regularizing large fully-connected layers within neural networks. When training with Dropout, a randomly selected subset of activations are set to zero within each layer. DropConnect instead sets a randomly selected subset of weights within the network to zero. Each unit thus receives input from a random subset of units in the previous layer. We derive a bound on the generalization performance of both Dropout and DropConnect. We then evaluate DropConnect on a range of datasets, comparing to Dropout, and show state-of-the-art results on several image recoginition benchmarks can be obtained by aggregating multiple DropConnect-trained models.} } 
 
 @misc{haste2020,
  title  = {Haste: a fast, simple, and open RNN library},
  author = {Sharvil Nanavati},
  year   = 2020,
  month  = "Jan",
  howpublished = {\url{https://github.com/lmnt-com/haste/}},
}

@misc{goodfellow2015qualitatively,
      title={Qualitatively characterizing neural network optimization problems}, 
      author={Ian J. Goodfellow and Oriol Vinyals and Andrew M. Saxe},
      year={2015},
      eprint={1412.6544},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}

@article{DBLP:journals/corr/abs-1809-05165,
  author    = {Siyue Wang and
               Xiao Wang and
               Pu Zhao and
               Wujie Wen and
               David R. Kaeli and
               Sang Peter Chin and
               Xue Lin},
  title     = {Defensive Dropout for Hardening Deep Neural Networks under Adversarial
               Attacks},
  journal   = {CoRR},
  volume    = {abs/1809.05165},
  year      = {2018},
  url       = {http://arxiv.org/abs/1809.05165},
  archivePrefix = {arXiv},
  eprint    = {1809.05165},
  timestamp = {Mon, 23 Sep 2019 17:13:21 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1809-05165.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{morris2020textattack,
      title={TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP}, 
      author={John X. Morris and Eli Lifland and Jin Yong Yoo and Jake Grigsby and Di Jin and Yanjun Qi},
      year={2020},
      eprint={2005.05909},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
